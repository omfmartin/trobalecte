{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tempfile\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from auxiliar import crear_ensemble_donadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "INPUT_FICHIER_WIKI_TEXTE = \"../donadas/brut/wikipedia_texte.csv\"\n",
    "INPUT_FICHER_WIKI_ETIQUETAS_MANUAL = \"../donadas/etiquetas/wikipedia_dialectes_manual.csv\"\n",
    "INPUT_FICHER_WIKI_ETIQUETAS_AUTOMATIC = \"../donadas/etiquetas/wikipedia_dialectes_automatic.csv\"\n",
    "INPUT_FICHER_WIKI_ETIQUETAS_ENTROPIA = \"../donadas/etiquetas/wikipedia_dialectes_entropia.csv\"\n",
    "INPUT_DORSIER_TOKENIZAIRE = \"../models/tokenizaire\"\n",
    "\n",
    "OUTPUT_TEXTE_ETIQUETAS_ENTRENAMENT = \"../donadas/processat/entrenament.csv\"\n",
    "OUTPUT_TEXTE_ETIQUETAS_TEST = \"../donadas/processat/test.csv\"\n",
    "OUTPUT_PREDICCIONS = \"../models/classificador/wikipedia_prediccions.csv\"\n",
    "OUTPUT_DORSIER_MODEL = \"../models/classificador/\"\n",
    "\n",
    "OPTIMIZACION_BAYESIANA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte_df = pd.read_csv(INPUT_FICHIER_WIKI_TEXTE)\n",
    "\n",
    "texte_etiquetas_df = crear_ensemble_donadas(\n",
    "    texte_df=texte_df,\n",
    "    lista_dialectes_df=[\n",
    "        pd.read_csv(INPUT_FICHER_WIKI_ETIQUETAS_MANUAL),\n",
    "        pd.read_csv(INPUT_FICHER_WIKI_ETIQUETAS_AUTOMATIC),\n",
    "        pd.read_csv(INPUT_FICHER_WIKI_ETIQUETAS_ENTROPIA),\n",
    "    ],\n",
    ")\n",
    "texte_etiquetas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte_etiquetas_entrenament, texte_etiquetas_test = train_test_split(texte_etiquetas_df, test_size=0.2)\n",
    "texte_etiquetas_entrenament.to_csv(OUTPUT_TEXTE_ETIQUETAS_ENTRENAMENT, index=False)\n",
    "texte_etiquetas_test.to_csv(OUTPUT_TEXTE_ETIQUETAS_TEST, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear sac a mots\n",
    "tokenizaire = GPT2Tokenizer.from_pretrained(\n",
    "    INPUT_DORSIER_TOKENIZAIRE,\n",
    "    do_lower_case=True,\n",
    ")\n",
    "tokens_entrenament = texte_etiquetas_entrenament[\"Texte\"].apply(tokenizaire.tokenize).apply(lambda x: ' '.join(x))\n",
    "tokens_test = texte_etiquetas_test[\"Texte\"].apply(tokenizaire.tokenize).apply(lambda x: ' '.join(x))\n",
    "tokens_tot = texte_df[\"Texte\"].apply(tokenizaire.tokenize).apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorizador = CountVectorizer()\n",
    "bow_entrenament = vectorizador.fit_transform(tokens_entrenament)\n",
    "bow_test = vectorizador.transform(tokens_test)\n",
    "bow_tot = vectorizador.transform(tokens_tot)\n",
    "\n",
    "# Codificar los dialèctes\n",
    "codificador_ordinal = OrdinalEncoder()\n",
    "etiquetas_entrenament = codificador_ordinal.fit_transform(texte_etiquetas_entrenament[[\"Dialecte\"]]).astype(int)\n",
    "etiquetas_test = codificador_ordinal.transform(texte_etiquetas_test[[\"Dialecte\"]]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melhor_params = {}\n",
    "if OPTIMIZACION_BAYESIANA:\n",
    "    def evaluar(max_depth, learning_rate, n_estimators, gamma, min_child_weight, subsample, colsample_bytree):\n",
    "        kf = KFold(n_splits=3)\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_index, valid_index in kf.split(bow_entrenament):\n",
    "            \n",
    "            xgb = XGBClassifier(\n",
    "                max_depth=int(max_depth),\n",
    "                learning_rate=learning_rate,\n",
    "                n_estimators=int(n_estimators),\n",
    "                gamma=gamma,\n",
    "                min_child_weight=min_child_weight,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "            )\n",
    "            xgb.fit(\n",
    "                bow_entrenament[train_index],\n",
    "                etiquetas_entrenament[train_index],\n",
    "                sample_weight=compute_sample_weight(\"balanced\", etiquetas_entrenament[train_index]),\n",
    "                verbose=False,\n",
    "            )\n",
    "            etiquetas_predichas = xgb.predict(bow_entrenament[valid_index])\n",
    "            score = f1_score(etiquetas_entrenament[valid_index], etiquetas_predichas, average=\"macro\")\n",
    "            f1_scores.append(score)\n",
    "\n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "    limits = {\n",
    "        \"max_depth\": (3, 12),\n",
    "        \"learning_rate\": (0.01, 0.5),\n",
    "        \"n_estimators\": (50, 200),\n",
    "        \"gamma\": (0, 0.5),\n",
    "        \"min_child_weight\": (1, 10),\n",
    "        \"subsample\": (0.5, 1),\n",
    "        \"colsample_bytree\": (0.5, 1),\n",
    "    }\n",
    "\n",
    "    optimizaire = BayesianOptimization(f=evaluar, pbounds=limits, random_state=63, verbose=2)\n",
    "\n",
    "    optimizaire.maximize(init_points=10, n_iter=20)\n",
    "\n",
    "    melhor_params = optimizaire.max[\"params\"]\n",
    "    melhor_params[\"max_depth\"] = int(melhor_params[\"max_depth\"])\n",
    "    melhor_params[\"n_estimators\"] = int(melhor_params[\"n_estimators\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"XGBoost\")\n",
    "os.makedirs(\"mlruns\", exist_ok = True) \n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Entrenar e gardar lo darrièr model\n",
    "    xgb_final = XGBClassifier(\n",
    "        **melhor_params,\n",
    "    )\n",
    "\n",
    "    xgb_final.fit(\n",
    "        bow_entrenament,\n",
    "        etiquetas_entrenament,\n",
    "        sample_weight=compute_sample_weight(\"balanced\", etiquetas_entrenament),\n",
    "    )\n",
    "    for nom_param, valor_param in melhor_params.items():\n",
    "        mlflow.log_param(nom_param, valor_param)\n",
    "\n",
    "    # Predire e evaluar lo model\n",
    "    y_pred_test = xgb_final.predict(bow_test).reshape(-1, 1)\n",
    "\n",
    "    # Transformar las etiquetas numericas en etiquetas textualas\n",
    "    dialecte_obs_test = codificador_ordinal.inverse_transform(etiquetas_test)\n",
    "    dialecte_pred_test = codificador_ordinal.inverse_transform(y_pred_test)\n",
    "\n",
    "    # Exactitud, Precision, Rappel, F1\n",
    "    exactitud = accuracy_score(dialecte_obs_test, dialecte_pred_test)\n",
    "    precision, rappel, f1, _ = precision_recall_fscore_support(\n",
    "        dialecte_obs_test,\n",
    "        dialecte_pred_test,\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    print(f\"Exactitud: {exactitud}\\nPrecision: {precision}\\nRappel: {rappel}\\nF1: {f1}\")\n",
    "    mlflow.log_metric(\"exactitud\", exactitud)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"rappel\", rappel)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "\n",
    "    # Matritz de confusion\n",
    "    matritz_confusion = confusion_matrix(dialecte_obs_test, dialecte_pred_test)\n",
    "    matritz_confusion_df = pd.DataFrame(\n",
    "        matritz_confusion,\n",
    "        index=codificador_ordinal.categories_[0],\n",
    "        columns=codificador_ordinal.categories_[0],\n",
    "    )\n",
    "    fig = sns.heatmap(np.log(matritz_confusion_df + 1), annot=matritz_confusion_df, cbar=None, cmap=\"icefire\")\n",
    "    with tempfile.NamedTemporaryFile(suffix='.png') as temp_file:\n",
    "        plt.savefig(temp_file.name)\n",
    "        mlflow.log_artifact(temp_file.name, artifact_path=\"images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(OUTPUT_DORSIER_MODEL) / Path('vectorizador.pkl'), 'wb') as f:\n",
    "    pkl.dump(vectorizador, f)\n",
    "\n",
    "with open(Path(OUTPUT_DORSIER_MODEL) / Path('codificador_ordinal.pkl'), 'wb') as f:\n",
    "    pkl.dump(codificador_ordinal, f)\n",
    "\n",
    "with open(Path(OUTPUT_DORSIER_MODEL) / Path('classificador.pkl'), 'wb') as f:\n",
    "    pkl.dump(xgb_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar prediccions per totes los articles\n",
    "y_pred_tot = xgb_final.predict(bow_tot).reshape(-1, 1)\n",
    "y_prob_tot = xgb_final.predict_proba(bow_tot)\n",
    "entropia_tot = np.sum(-y_prob_tot * np.log(y_prob_tot), axis=1)\n",
    "resultats_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Article\": texte_df[\"Article\"],\n",
    "        \"Prediccion\": codificador_ordinal.inverse_transform(y_pred_tot).reshape(-1),\n",
    "        \"Entropia\": entropia_tot.reshape(-1),\n",
    "    }\n",
    ")\n",
    "resultats_df = pd.concat(\n",
    "    [resultats_df, pd.DataFrame(y_prob_tot, columns=codificador_ordinal.categories_[0])],\n",
    "    axis=1,\n",
    ")\n",
    "resultats_df = resultats_df.merge(texte_etiquetas_df[[\"Article\", \"Dialecte\"]], on=\"Article\", how=\"left\")\n",
    "resultats_df.to_csv(OUTPUT_PREDICCIONS, index=False)\n",
    "resultats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=resultats_df, x=\"Prediccion\", y=\"Entropia\", hue=\"Prediccion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_df.query('Dialecte.isnull() & Prediccion == \"lengadocian\"').sort_values(\"Entropia\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_df.query('Dialecte.isnull() & Prediccion == \"auvernhat\"').sort_values(\"Entropia\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte_etiquetas_df.groupby(\"Dialecte\")[\"Article\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_df.groupby(\"Prediccion\")[\"Article\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {v: k for k, v in vectorizador.vocabulary_.items()}\n",
    "vocab_list = [vocab[i] for i in range(len(vocab))]\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explicador = shap.TreeExplainer(xgb_final)\n",
    "shap_df = pd.DataFrame(explicador(bow_test), index=texte_etiquetas_test[\"Article\"], columns=vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame({\"FI\": xgb_final.feature_importances_})\n",
    "fi_df[\"TokenIndex\"] = list(range(0, fi_df.shape[0]))\n",
    "fi_df[\"Token\"] = fi_df[\"TokenIndex\"].map(lambda x: vocab[x])\n",
    "fi_df.sort_values('FI', ascending=False).head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trobalecte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
